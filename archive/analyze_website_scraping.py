#!/usr/bin/env python3
"""
Analyze RentFaster website structure to determine best scraping method
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
import re

def analyze_rentfaster_website(city_url='https://www.rentfaster.ca/on/toronto/rentals'):
    """Analyze how the website loads and displays listings"""
    
    chrome_options = Options()
    chrome_options.add_argument('--disable-blink-features=AutomationControlled')
    chrome_options.add_experimental_option('excludeSwitches', ['enable-automation'])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    driver.execute_script('Object.defineProperty(navigator, "webdriver", {get: () => undefined})')
    
    print('=' * 80)
    print('RENTFASTER WEBSITE SCRAPING ANALYSIS')
    print('=' * 80)
    print(f'\nAnalyzing: {city_url}')
    
    # Load page
    driver.get(city_url)
    print('\n‚è≥ Waiting for Cloudflare...')
    time.sleep(15)
    
    # Method 1: Check for listing cards with specific classes
    print('\n' + '=' * 80)
    print('METHOD 1: Direct HTML Parsing (Listing Cards)')
    print('=' * 80)
    
    listing_selectors = [
        ('div.listing', 'Standard listing div'),
        ('div[data-ref-id]', 'Elements with ref-id attribute'),
        ('div.listing-item', 'Listing item divs'),
        ('[ng-repeat*="listing"]', 'AngularJS ng-repeat'),
        ('div[class*="listing-"]', 'Any div with listing- class'),
    ]
    
    for selector, desc in listing_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f'\n‚úì {desc}: {len(elements)} found')
                print(f'  Selector: {selector}')
                
                # Analyze first element
                if len(elements) > 0:
                    first = elements[0]
                    text_sample = first.text[:100].replace('\n', ' ')
                    print(f'  Sample text: "{text_sample}..."')
                    
                    # Check for ref_id
                    ref_id = first.get_attribute('data-ref-id')
                    if ref_id:
                        print(f'  ‚úì Has ref_id attribute: {ref_id}')
                    
                    # Check for link
                    links = first.find_elements(By.TAG_NAME, 'a')
                    if links:
                        href = links[0].get_attribute('href')
                        print(f'  ‚úì Has link: {href[:50]}...')
        except Exception as e:
            print(f'\n‚úó {desc}: Error - {str(e)[:50]}')
    
    # Method 2: Check for embedded JSON data
    print('\n' + '=' * 80)
    print('METHOD 2: Embedded JSON Data Extraction')
    print('=' * 80)
    
    try:
        # Check all script tags for JSON data
        scripts = driver.find_elements(By.TAG_NAME, 'script')
        print(f'\nFound {len(scripts)} script tags, analyzing...')
        
        listings_found = []
        
        for i, script in enumerate(scripts):
            try:
                content = script.get_attribute('innerHTML')
                if not content or len(content) < 100:
                    continue
                    
                # Check for listings array
                if 'listings' in content.lower() and 'ref_id' in content:
                    print(f'\n‚úì Script {i+1}: Contains listing data!')
                    
                    # Try to extract JSON
                    # Look for patterns like: var listings = [...] or listings: [...]
                    patterns = [
                        r'listings\s*[=:]\s*(\[.*?\])',
                        r'"listings"\s*:\s*(\[.*?\])',
                        r'listings:\s*(\[.*?\])',
                    ]
                    
                    for pattern in patterns:
                        matches = re.findall(pattern, content, re.DOTALL)
                        if matches:
                            try:
                                # Try to parse as JSON
                                data = json.loads(matches[0])
                                listings_found.extend(data)
                                print(f'  ‚Üí Extracted {len(data)} listings from JSON array')
                                break
                            except:
                                pass
                    
                    # Alternative: count ref_id occurrences
                    ref_ids = re.findall(r'"ref_id"\s*:\s*(\d+)', content)
                    if ref_ids:
                        print(f'  ‚Üí Found {len(ref_ids)} ref_id mentions')
                        print(f'  ‚Üí Sample IDs: {ref_ids[:5]}')
                        
            except:
                pass
        
        if listings_found:
            print(f'\n‚úÖ Total listings extracted: {len(listings_found)}')
            print(f'Sample listing: {json.dumps(listings_found[0], indent=2)[:200]}...')
        else:
            print('\n‚ö†Ô∏è  Could not extract structured JSON data')
            
    except Exception as e:
        print(f'\n‚úó Error analyzing scripts: {e}')
    
    # Method 3: Check for infinite scroll / lazy loading
    print('\n' + '=' * 80)
    print('METHOD 3: Infinite Scroll / Lazy Loading')
    print('=' * 80)
    
    try:
        # Count initial listings
        initial_cards = len(driver.find_elements(By.CSS_SELECTOR, '[class*="listing"]'))
        print(f'\nInitial visible elements: {initial_cards}')
        
        # Scroll and wait
        print('Scrolling down to trigger lazy loading...')
        for i in range(5):
            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')
            time.sleep(2)
            current_cards = len(driver.find_elements(By.CSS_SELECTOR, '[class*="listing"]'))
            if current_cards > initial_cards:
                print(f'  Scroll {i+1}: {current_cards} elements (+{current_cards - initial_cards})')
                initial_cards = current_cards
            else:
                print(f'  Scroll {i+1}: No new elements loaded')
        
        final_cards = len(driver.find_elements(By.CSS_SELECTOR, '[class*="listing"]'))
        print(f'\n‚úì Final count: {final_cards} elements')
        
        if final_cards > 100:
            print('  ‚ö†Ô∏è  This suggests infinite scroll is working')
            print('  ‚Üí Would need to scroll through entire page to get all 4,210 listings')
            print('  ‚Üí Estimated scrolls needed: ~84 (4210/50 per scroll)')
            
    except Exception as e:
        print(f'\n‚úó Error testing scroll: {e}')
    
    # Method 4: Check if there's a "Load More" button
    print('\n' + '=' * 80)
    print('METHOD 4: Load More / Pagination')
    print('=' * 80)
    
    try:
        load_more_selectors = [
            'button:contains("Load More")',
            'button:contains("Show More")',
            'a:contains("Next")',
            '.pagination a',
            '[class*="load-more"]',
            '[class*="show-more"]',
        ]
        
        for selector in load_more_selectors:
            try:
                if ':contains' in selector:
                    # XPath for contains
                    xpath = selector.replace('button:contains("', '//button[contains(text(), "').replace('")', '")]')
                    elements = driver.find_elements(By.XPATH, xpath)
                else:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    
                if elements:
                    print(f'\n‚úì Found: {selector}')
                    print(f'  Count: {len(elements)}')
            except:
                pass
                
    except Exception as e:
        print(f'\n‚úó Error checking pagination: {e}')
    
    driver.quit()
    
    # Summary and recommendations
    print('\n' + '=' * 80)
    print('SUMMARY & RECOMMENDATIONS')
    print('=' * 80)
    
    print('''
üéØ FINDING: RentFaster uses INFINITE SCROLL for listing display

üìä Current Situation:
   - API returns: ~1,733 listings (37 pages)
   - Website shows: 4,210 listings
   - Gap: 2,477 missing listings (59%)

üí° Alternative Scraping Methods:

1. **INFINITE SCROLL SCRAPING** (Most Complete)
   Pros: Gets all 4,210 listings
   Cons: Slow (need to scroll ~84 times), resource intensive
   Time: ~10-15 minutes per city
   
   Implementation:
   - Load city page
   - Scroll down repeatedly
   - Extract listing cards after each scroll
   - Parse ref_id and details from HTML
   - Stop when no new listings appear

2. **EMBEDDED JSON EXTRACTION** (Medium)
   Pros: Faster than scrolling
   Cons: May not have all listings, needs parsing
   Time: ~2-3 minutes per city
   
   Implementation:
   - Load city page
   - Extract JSON from <script> tags
   - Parse listing data
   - May still miss some listings

3. **HYBRID APPROACH** (Recommended)
   Pros: Balance of speed and completeness
   Cons: More complex
   Time: ~5-7 minutes per city
   
   Implementation:
   - Use API for initial bulk fetch (fast)
   - Use infinite scroll for gap listings
   - Deduplicate based on ref_id
   - Get most complete dataset

üöÄ RECOMMENDATION: Use API data (13,181 listings) + optional manual verification
   - API is fast and reliable
   - Covers major cities well
   - Good enough for most use cases
   - Can supplement with scroll for critical cities (Toronto, Ottawa)
''')

if __name__ == '__main__':
    analyze_rentfaster_website()
